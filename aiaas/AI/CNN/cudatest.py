import torch
import time
import numpy as np
import matplotlib.pyplot as plt

def check_cuda_availability():
    """CUDA ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸"""
    print(f"PyTorch ë²„ì „: {torch.__version__}")
    print(f"CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"CUDA ë²„ì „: {torch.version.cuda}")
        print(f"GPU ê°œìˆ˜: {torch.cuda.device_count()}")
        print(f"í˜„ì¬ GPU: {torch.cuda.get_device_name(0)}")
    print("-" * 50)

def format_time(seconds):
    """ì‹œê°„ì„ ì½ê¸° ì‰¬ìš´ í˜•íƒœë¡œ í¬ë§·íŒ…"""
    if seconds >= 1:
        return f"{seconds:.3f}ì´ˆ"
    elif seconds >= 0.001:
        return f"{seconds*1000:.1f}ms"
    else:
        return f"{seconds*1000000:.1f}Î¼s"

def format_time_for_plot(times):
    """í”Œë¡¯ìš© ì‹œê°„ ë‹¨ìœ„ ìë™ ì„ íƒ ë° ë³€í™˜"""
    max_time = max(times)
    if max_time >= 1:
        return [t for t in times], "ì´ˆ"
    elif max_time >= 0.001:
        return [t * 1000 for t in times], "ë°€ë¦¬ì´ˆ"
    else:
        return [t * 1000000 for t in times], "ë§ˆì´í¬ë¡œì´ˆ"

def benchmark_tensor_operations(size, iterations=100):
    """í…ì„œ ì—°ì‚° ë²¤ì¹˜ë§ˆí¬"""
    print(f"\nğŸ“Š í–‰ë ¬ í¬ê¸°: {size:,} Ã— {size:,} (ë°˜ë³µ: {iterations}íšŒ)")
    print("â”€" * 60)
    
    # CPU ë²¤ì¹˜ë§ˆí¬
    print("ğŸ”„ CPUì—ì„œ í–‰ë ¬ê³±ì…ˆ ì‹¤í–‰ ì¤‘...")
    a_cpu = torch.randn(size, size)
    b_cpu = torch.randn(size, size)
    
    start_time = time.time()
    for _ in range(iterations):
        c_cpu = torch.mm(a_cpu, b_cpu)
    cpu_time = time.time() - start_time
    
    avg_cpu_time = cpu_time / iterations
    
    # CUDA ë²¤ì¹˜ë§ˆí¬ (CUDA ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°)
    if torch.cuda.is_available():
        print("ğŸš€ GPUì—ì„œ í–‰ë ¬ê³±ì…ˆ ì‹¤í–‰ ì¤‘...")
        device = torch.device('cuda')
        a_gpu = torch.randn(size, size, device=device)
        b_gpu = torch.randn(size, size, device=device)
        
        # GPU warm-up
        for _ in range(10):
            torch.mm(a_gpu, b_gpu)
        torch.cuda.synchronize()
        
        start_time = time.time()
        for _ in range(iterations):
            c_gpu = torch.mm(a_gpu, b_gpu)
        torch.cuda.synchronize()
        gpu_time = time.time() - start_time
        
        avg_gpu_time = gpu_time / iterations
        speedup = cpu_time / gpu_time
        
        # ê²°ê³¼ ì¶œë ¥
        print("\nğŸ“ˆ ì‹¤í–‰ ê²°ê³¼:")
        print(f"   ğŸ’» CPU ì´ ì‹œê°„     : {format_time(cpu_time)}")
        print(f"   ğŸ’» CPU í‰ê·  ì‹œê°„   : {format_time(avg_cpu_time)}")
        print(f"   ğŸ® GPU ì´ ì‹œê°„     : {format_time(gpu_time)}")
        print(f"   ğŸ® GPU í‰ê·  ì‹œê°„   : {format_time(avg_gpu_time)}")
        print(f"   âš¡ ì†ë„ í–¥ìƒ       : {speedup:.1f}ë°° ë¹ ë¦„")
        
        return cpu_time, gpu_time
    else:
        print("\nâš ï¸  CUDAë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print(f"   ğŸ’» CPU ì´ ì‹œê°„     : {format_time(cpu_time)}")
        print(f"   ğŸ’» CPU í‰ê·  ì‹œê°„   : {format_time(avg_cpu_time)}")
        return cpu_time, None

def benchmark_neural_network(batch_size=64, input_size=784, hidden_size=512, 
                           output_size=10, iterations=100):
    """ì‹ ê²½ë§ ìˆœì „íŒŒ ë²¤ì¹˜ë§ˆí¬"""
    print(f"\nì‹ ê²½ë§ ë²¤ì¹˜ë§ˆí¬ (ë°°ì¹˜ í¬ê¸°: {batch_size})")
    
    # ê°„ë‹¨í•œ MLP ëª¨ë¸ ì •ì˜
    class SimpleMLP(torch.nn.Module):
        def __init__(self, input_size, hidden_size, output_size):
            super(SimpleMLP, self).__init__()
            self.fc1 = torch.nn.Linear(input_size, hidden_size)
            self.fc2 = torch.nn.Linear(hidden_size, hidden_size)
            self.fc3 = torch.nn.Linear(hidden_size, output_size)
            self.relu = torch.nn.ReLU()
        
        def forward(self, x):
            x = self.relu(self.fc1(x))
            x = self.relu(self.fc2(x))
            x = self.fc3(x)
            return x
    
    # CPU ë²¤ì¹˜ë§ˆí¬
    model_cpu = SimpleMLP(input_size, hidden_size, output_size)
    data_cpu = torch.randn(batch_size, input_size)
    
    start_time = time.time()
    for _ in range(iterations):
        output_cpu = model_cpu(data_cpu)
    cpu_time = time.time() - start_time
    
    print(f"CPU ì‹ ê²½ë§ ìˆœì „íŒŒ ì‹œê°„: {format_time(cpu_time)}")
    
    # CUDA ë²¤ì¹˜ë§ˆí¬
    if torch.cuda.is_available():
        model_gpu = SimpleMLP(input_size, hidden_size, output_size).cuda()
        data_gpu = torch.randn(batch_size, input_size, device='cuda')
        
        # GPU warm-up
        for _ in range(10):
            model_gpu(data_gpu)
        torch.cuda.synchronize()
        
        start_time = time.time()
        for _ in range(iterations):
            output_gpu = model_gpu(data_gpu)
        torch.cuda.synchronize()
        gpu_time = time.time() - start_time
        
        print(f"GPU ì‹ ê²½ë§ ìˆœì „íŒŒ ì‹œê°„: {format_time(gpu_time)}")
        print(f"ì†ë„ í–¥ìƒ: {cpu_time/gpu_time:.1f}ë°°")
        
        return cpu_time, gpu_time
    else:
        return cpu_time, None

def comprehensive_benchmark():
    """ì¢…í•©ì ì¸ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
    check_cuda_availability()
    
    # ë‹¤ì–‘í•œ í¬ê¸°ì˜ í…ì„œë¡œ ë²¤ì¹˜ë§ˆí¬ (í¬ê¸° ì¦ê°€)
    sizes = [256, 512, 768, 1024, 1536, 2048, 3072, 4096, 5120, 6144, 7168, 8192]
    cpu_times = []
    gpu_times = []
    
    print("=" * 80)
    print("ğŸ”¥ í–‰ë ¬ ê³±ì…ˆ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬")
    print("=" * 80)
    
    for size in sizes:
        cpu_time, gpu_time = benchmark_tensor_operations(size, iterations=30)
        cpu_times.append(cpu_time)
        if gpu_time is not None:
            gpu_times.append(gpu_time)
    
    # ì‹ ê²½ë§ ë²¤ì¹˜ë§ˆí¬
    print("\n" + "=" * 60)
    print("ì‹ ê²½ë§ ë²¤ì¹˜ë§ˆí¬")
    print("=" * 60)
    
    batch_sizes = [16, 32, 48, 64, 96, 128, 192, 256, 384, 512]
    nn_cpu_times = []
    nn_gpu_times = []
    
    for batch_size in batch_sizes:
        print(f"\në°°ì¹˜ í¬ê¸° {batch_size} í…ŒìŠ¤íŠ¸:")
        cpu_time, gpu_time = benchmark_neural_network(batch_size=batch_size, iterations=50)
        nn_cpu_times.append(cpu_time)
        if gpu_time is not None:
            nn_gpu_times.append(gpu_time)
    
    # ê²°ê³¼ ì‹œê°í™”
    if torch.cuda.is_available() and gpu_times:
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))
        plt.rc('font', family='Malgun Gothic')
        
        # í–‰ë ¬ ê³±ì…ˆ ê²°ê³¼ - ì ˆëŒ€ ì‹œê°„
        cpu_plot_times, cpu_time_unit = format_time_for_plot(cpu_times)
        gpu_plot_times, gpu_time_unit = format_time_for_plot(gpu_times)
        
        ax1.semilogy(sizes, cpu_plot_times, 'b-o', label=f'CPU ({cpu_time_unit})', linewidth=2, markersize=6)
        ax1.semilogy(sizes, gpu_plot_times, 'r-o', label=f'GPU ({gpu_time_unit})', linewidth=2, markersize=6)
        ax1.set_xlabel('í–‰ë ¬ í¬ê¸°', fontsize=12)
        ax1.set_ylabel(f'ì‹¤í–‰ ì‹œê°„ ({cpu_time_unit})', fontsize=12)
        ax1.set_title('í–‰ë ¬ ê³±ì…ˆ ì ˆëŒ€ ì‹¤í–‰ì‹œê°„ ë¹„êµ', fontsize=14, fontweight='bold')
        ax1.legend(fontsize=11)
        ax1.grid(True, alpha=0.3)
        ax1.set_xticks(sizes[::2])  # ì¼ë¶€ xì¶• ë¼ë²¨ë§Œ í‘œì‹œ
        
        # í–‰ë ¬ ê³±ì…ˆ ì†ë„ í–¥ìƒ ë¹„ìœ¨
        speedups = [cpu/gpu for cpu, gpu in zip(cpu_times, gpu_times)]
        ax2.plot(sizes, speedups, 'g-o', linewidth=3, markersize=7, color='darkgreen')
        ax2.set_xlabel('í–‰ë ¬ í¬ê¸°', fontsize=12)
        ax2.set_ylabel('ì†ë„ í–¥ìƒ (ë°°)', fontsize=12)
        ax2.set_title('í–‰ë ¬ ê³±ì…ˆ GPU ì†ë„ í–¥ìƒ', fontsize=14, fontweight='bold')
        ax2.grid(True, alpha=0.3)
        ax2.set_xticks(sizes[::2])
        # ì†ë„ í–¥ìƒ ê°’ì„ ê° ì ì— í‘œì‹œ
        for i, (x, y) in enumerate(zip(sizes, speedups)):
            if i % 2 == 0:  # ì¼ë¶€ë§Œ í‘œì‹œ
                ax2.annotate(f'{y:.1f}x', (x, y), textcoords="offset points", 
                           xytext=(0,10), ha='center', fontsize=9, fontweight='bold')
        
        # ì‹ ê²½ë§ ê²°ê³¼ - ì ˆëŒ€ ì‹œê°„
        nn_cpu_plot_times, nn_cpu_time_unit = format_time_for_plot(nn_cpu_times)
        nn_gpu_plot_times, nn_gpu_time_unit = format_time_for_plot(nn_gpu_times)
        
        ax3.plot(batch_sizes, nn_cpu_plot_times, 'b-o', label=f'CPU ({nn_cpu_time_unit})', linewidth=2, markersize=6)
        ax3.plot(batch_sizes, nn_gpu_plot_times, 'r-o', label=f'GPU ({nn_gpu_time_unit})', linewidth=2, markersize=6)
        ax3.set_xlabel('ë°°ì¹˜ í¬ê¸°', fontsize=12)
        ax3.set_ylabel(f'ì‹¤í–‰ ì‹œê°„ ({nn_cpu_time_unit})', fontsize=12)
        ax3.set_title('ì‹ ê²½ë§ ìˆœì „íŒŒ ì ˆëŒ€ ì‹¤í–‰ì‹œê°„ ë¹„êµ', fontsize=14, fontweight='bold')
        ax3.legend(fontsize=11)
        ax3.grid(True, alpha=0.3)
        ax3.set_xticks(batch_sizes[::2])
        
        # ì‹ ê²½ë§ ì†ë„ í–¥ìƒ ë¹„ìœ¨
        nn_speedups = [cpu/gpu for cpu, gpu in zip(nn_cpu_times, nn_gpu_times)]
        ax4.plot(batch_sizes, nn_speedups, 'g-o', linewidth=3, markersize=7, color='purple')
        ax4.set_xlabel('ë°°ì¹˜ í¬ê¸°', fontsize=12)
        ax4.set_ylabel('ì†ë„ í–¥ìƒ (ë°°)', fontsize=12)
        ax4.set_title('ì‹ ê²½ë§ GPU ì†ë„ í–¥ìƒ', fontsize=14, fontweight='bold')
        ax4.grid(True, alpha=0.3)
        ax4.set_xticks(batch_sizes[::2])
        # ì†ë„ í–¥ìƒ ê°’ì„ ê° ì ì— í‘œì‹œ
        for i, (x, y) in enumerate(zip(batch_sizes, nn_speedups)):
            if i % 2 == 0:  # ì¼ë¶€ë§Œ í‘œì‹œ
                ax4.annotate(f'{y:.1f}x', (x, y), textcoords="offset points", 
                           xytext=(0,10), ha='center', fontsize=9, fontweight='bold')
        
        plt.tight_layout(pad=3.0)
        plt.show()
        
        # ì†ë„ í–¥ìƒ ìš”ì•½
        print("\n" + "=" * 80)
        print("ğŸ“‹ ì„±ëŠ¥ ìš”ì•½ ë³´ê³ ì„œ")
        print("=" * 80)
        speedups = [cpu/gpu for cpu, gpu in zip(cpu_times, gpu_times)]
        nn_speedups = [cpu/gpu for cpu, gpu in zip(nn_cpu_times, nn_gpu_times)]
        
        print("\nğŸ”¢ í–‰ë ¬ ê³±ì…ˆ ì„±ëŠ¥:")
        print("   í¬ê¸°        CPU ì‹œê°„      GPU ì‹œê°„      ì†ë„í–¥ìƒ")
        print("   " + "â”€" * 50)
        for size, cpu_t, gpu_t, speedup in zip(sizes, cpu_times, gpu_times, speedups):
            print(f"   {size:,}Ã—{size:,}    {format_time(cpu_t):>10}    {format_time(gpu_t):>10}    {speedup:>6.1f}ë°°")
        
        print(f"\nğŸ“Š í‰ê·  ì†ë„ í–¥ìƒ: {np.mean(speedups):.1f}ë°°")
        
        print("\nğŸ§  ì‹ ê²½ë§ ì„±ëŠ¥:")
        print("   ë°°ì¹˜í¬ê¸°    CPU ì‹œê°„      GPU ì‹œê°„      ì†ë„í–¥ìƒ")
        print("   " + "â”€" * 50)
        for batch_size, cpu_t, gpu_t, speedup in zip(batch_sizes, nn_cpu_times, nn_gpu_times, nn_speedups):
            print(f"   {batch_size:>8}    {format_time(cpu_t):>10}    {format_time(gpu_t):>10}    {speedup:>6.1f}ë°°")
        
        print(f"\nğŸ“ˆ í‰ê·  ì†ë„ í–¥ìƒ: {np.mean(nn_speedups):.1f}ë°°")

def memory_usage_comparison():
    """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¹„êµ"""
    if not torch.cuda.is_available():
        print("CUDAë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ì–´ ë©”ëª¨ë¦¬ ë¹„êµë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
        return
    
    print("\n" + "=" * 60)
    print("ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¹„êµ")
    print("=" * 60)
    
    sizes = [1000, 2000, 4000, 8000]
    
    for size in sizes:
        # GPU ë©”ëª¨ë¦¬ ì´ˆê¸°í™”
        torch.cuda.empty_cache()
        torch.cuda.reset_peak_memory_stats()
        
        # GPUì—ì„œ í…ì„œ ìƒì„±
        tensor_gpu = torch.randn(size, size, device='cuda')
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì •
        memory_allocated = torch.cuda.memory_allocated() / 1024**2  # MB
        memory_reserved = torch.cuda.memory_reserved() / 1024**2   # MB
        
        print(f"í¬ê¸° {size}x{size}:")
        print(f"  í• ë‹¹ëœ ë©”ëª¨ë¦¬: {memory_allocated:.2f} MB")
        print(f"  ì˜ˆì•½ëœ ë©”ëª¨ë¦¬: {memory_reserved:.2f} MB")
        
        del tensor_gpu
        torch.cuda.empty_cache()

if __name__ == "__main__":
    # ì „ì²´ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
    comprehensive_benchmark()
    
    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¹„êµ
    memory_usage_comparison()
    
    print("\nğŸ‰ ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ!")
    print("ğŸ’¡ íŒ: GPU ì„±ëŠ¥ì€ í–‰ë ¬ í¬ê¸°ê°€ í´ìˆ˜ë¡ ë” í° í–¥ìƒì„ ë³´ì…ë‹ˆë‹¤.")